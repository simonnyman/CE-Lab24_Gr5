\documentclass[conference]{IEEEtran}
% \IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Computer Technology Project I\\}

\author{\IEEEauthorblockN{Simon Nyman}
\IEEEauthorblockA{\textit{Dept. of Electrical and Computer Engineering} \\
\textit{Aarhus University}\\
Studentnumber: 202305077\\}
\and
\IEEEauthorblockN{Jakob Palm}
\IEEEauthorblockA{\textit{Dept. of Electrical and Computer Engineering} \\
\textit{Aarhus University}\\
Studentnumber: 202307244\\}
}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}
The goal for this project was to design a down-scaled version of a Search And Rescue (SAR) robot, used to rescue victims in the aftermaths of natural disasters.
The robot will imitate a real world scenario by, autonomously navigating an obstacle course and distinguishing between different color markings, representing potential victims. To achieved this we use the TurtleBot3 Burger Robot equipped with different sensors.
All the software will be implmented in python using the Robot Operating Software framework (ROS). For the sensors we will be using a LiDAR sensor capable of measuring distances in a 360 degree view, and an RGB-sensor which differentiates between red, green and blue colors. 
The overall structure of the project was split into four parts:
\begin{itemize}
    \item Understanding the basic concepts of robotics programming, including the see-think- act cycle \href{sec:STAC}{Fig. 1}. Reading and processing data from a RGB and range sensor.
    \item Setting up a virtual machine, and working with ROS through a beginner's tutorial.
    \item Developing the robot navigation program, and working with ROS on the Raspberry PI .
    \item Optimizing the robot navigation and integrating code components for the RGB and LEDs
\end{itemize} 
Finally the performance of the robot would be assessed based on three factors: Average speed, number of collisions and "victims" found.

\section{Specifications}
Our Search and Rescue implementation used a turtlebot3 robot, specifically the burger configuration with dimensions 138mm x 178mm x 192mm (L $\times$ W $\times$ H)\cite{b1}.
On the turtlebot3, a Raspberry Pi 3 model B+ and an Arduino are connected in order to process the incoming external signals from the various sensors.
Furthermore, the robot has two motors attached, one for each wheel. From the specification it is evident that the robot has a maximal linear velocity of 0.22 m/s
and a maximum rotational velocity of 2.84 rad/s. In order keep the robot as agile as possible with no wires attached, we used an 800 mA Li-Po battery and established a wireless connection, which will be specified further in a following section.
The specific RGB used is an ISL29125 low power, high sensitivity red, green and blue light sensor with SMBus compatibility\cite{b2}.
The LiDAR used is the LDS-01 version with a range of 120 mm to 3500 mm, capable of doing full 360 degree scans. \cite{b3}

\subsection{Software Setup}

\subsubsection{Network Configuration}
As mentioned, we wanted the robot to be able to move as freely as possible, meaning that connecting it to our machine via an ethernet cable was not the way to go.
Therefore, we established a wireless connection to the robot by changing the network configuration on the sd card, making it automatically connect to the IP-address of our machine.
This allowed for easy connection to the robot once a mobile hotspot was opened, as the robot was automatically allocated an IP-address.
With this configured, the robot would subscribe to our machine on which the required ROS environments (these will be specified in the nect section) would publish actuation data to the motors of the robot, enabling wireless navigation.

\subsubsection{ROS}

The ROS operating system proved to be very useful in regards to this project, as it provided a very useful framework for our specific problem.
The three main parts used were topics, messages, and nodes (rostopic, rosmsg, rosnode). These are all integral parts of the ROS framework, as each provides a unique functionality in abstracting the navigation.
Nodes are responsible for performing computations and communicate internally through topics, while messages are used for communicating with the external sensors.

In our case, we had three nodes running. One running roscore, which encapsulates the entire framwork, allowing the other nodes to run in the ROS environment.
The second node ran the basic functionality of the robot, such as managing the incoming sensor data and sending signals to the motors.
The third node was running the navigation code that we wrote. These communicated by the subsribe/publish model to send data back and forth, in order to control the navigation of the robot.
The nodes also contain debug information and allowed us to log the information we required for optimizing and reason about the relevant aspects of the code as we were testing it.


\section{Methodology and Design}
Throughout the project we have been using the trial and error approach, which allowed for an efficient overall workflow. 
This approach works well, when having to build and optimize code for hardware, as we would need to see if and how the code is working and reasoning about it on an abstract level would only get us so far.
Furthermore, this approach works in great synergy with the See-Think-Act Cycle we use for the robot, see \href{sec:STAC}{Fig. 1}.
We are able to edit the "see", "think" and "act" parts individually and get instant feedback by testing, as this approach to navigation is very straight forward to reason about, simplifying both the building and optimization processes.
\begin{figure}[htbp]
    \centerline{\includegraphics[width=1.0\columnwidth]{Pictures/STAC.png}}
    \caption{Robots See-Think-Act Cycle.}
    \label{sec:STAC}
    \end{figure}

\subsection{LiDAR}
The use of LiDAR readings is an integral part of the design, as the entire navigation logic depends upon these.
In our implementation, it had two use cases, one being the ability to read in which direction the nearest obstacle was present, if any, and the second being the ability to read the specific distance to said obstacle, which would determine what to do next.
As the LiDAR provides a 360 degree scan, we first had to figure out which angles to use and how these were organized within the LiDAR.
By firstly playing around with the angles, we figured out that they were organized in a way such that 0$^\circ$ was at the back of the robot, and then continued clockwise around the robot.
This meant that the front 180$^\circ$ were located in the interval $[90:270]$. How this was used in our final implementation will be elaborated on in the \textit{Navigation} section.
During initial testing of the LiDAR, we encountered a problem, as some angles seemed to not be working properly and would either give readings very close to 0, which resulted in the robot refusing to move, or infinitely large readings.
To fix this, we wrote a loop within the LiDAR scan function, which would iterate through the values in the angles being read and, if any angles were suffering from either of these problems, would fix them, as seen in \href{sec:lidar}{Fig. 2}.
If the value was infinitely large, it would set it to the max value of the LiDAR, which, as previously mentioned, is 3.5 meters.
In the case that an angle gave readings very close to 0, it would set this to 1 meter as this would have no impact on the other readings.
We had a third case, which checked whether the reading was indeed a number as it should, and would otherwise set it to 0 in which case it would be ignored as described.
With this configured, we additionally addded a few centimeters to each reading due to the innate imprecision of the LiDAR, which will be apparent later on.
\begin{figure}[htbp]
    \centerline{\includegraphics[width=0.6\columnwidth\hspace{-1.3cm}]{Pictures/LiDARhvid.png}}
    \caption{Ignoring the dead LiDAR angles.}
    \label{sec:lidar}
    \end{figure}

\subsection{Navigation}
The logic for the navigation was developed to excel on our performance criteria of limiting collision, while achieveing a high speed, which in turn will allow the robot to move fast through the obstacle course, thereby detecting as many "victims" as possible.
Our navigation is based on an object detecting strategy, were we made the robots actions dependent on where the closest object was located.
To distinguish between possible locations of an object, we had to split the LiDAR readings into both an angular interval and a distance within that interval.
For our final iteration of the navigation program, we split the angular intervals into 6 cones, each spanning 25 deegrees in front of the robot as illustrated in \href{sec:angles}{Fig. 3}. 
As the navigation is based around the concept of avoiding objects obstructing the path of the robot, we "ignored" the 180 deegrees behind the robot, by making the robot move forward in full speed.
The same concept is applied to the two remaining 15 deegree cones of the forward facing 180 deegrees, which enables the robot to run parralel to an object without turning away.
\begin{figure}[htbp]
    \centerline{\includegraphics[width=0.9\columnwidth]{Pictures/LiDAR Angels.png}}
    \caption{LiDAR zones illustration. Please note that the angles of the cones are slightly distorted}
    \label{sec:angles}
    \end{figure}

When the the angle interval of the nearest object is registered, the navigation program would then evaluate the distance to the obstacle.
If the obstacle is registered to be within 30 centimeters, the robot would make a slight preventive turn while keeping the linear speed high.
If the object is within 22 centimeters, the robot would turn hard while lowering the linear speed drasticly.
In addition if an obstacle should be registered within 10 centimeters of the LiDAR a collision would be registered.
An illustration of the distances within the cones is provided in \href{sec:distances}{Fig. 4}.
The intervals and distances were both determined through our continous testing process, which we elaborate further on in the \textit{Experimentation and Testing} section.
\begin{figure}[htbp]
    \centerline{\includegraphics[width=0.9\columnwidth]{Pictures/LiDAR Distances.png}}
    \caption{LiDAR distances illustration. Please note that the angle of the cones and distances are slightly distorted}
    \label{sec:distances}
    \end{figure}

The thought behind this approach to our logic design, was to use the medium distance to locate objects early and then make the robot take preventive action.
Locating the obstacles early allows the robot to make a series of smaller turns, where the linear speed is kept high. The concept is visualized in \href{sec:distances}{Fig. 5}
The close distance was implemented to be used in situations where harder turns were needed to avoid a collision. A potential example could be a dead end, as illustrated in \href{sec:distances}{Fig. 6}
\begin{figure}[htbp]
    \centerline{\includegraphics[width=0.9\columnwidth]{Pictures/Medium Distance Aviodance.png}}
    \caption{Illustration of the robot using the medium distance in different angle intervals to move fast through a potential obstacle course scenario.}
    \label{sec:medium aviodance}
    \end{figure}
\begin{figure}[htbp]
    \centerline{\includegraphics[width=0.9\columnwidth]{Pictures/Close Distance Avoidance.png}}
    \caption{Illustration of a scenario where the robot would use the close distance.}
    \label{sec:close aviodance}
    \end{figure}
The way we implemented this navigation logic can be illustarated by the flow chart in \href{sec:distances}{Fig. 7}. 
First we would locate if there is an obstacle in our LiDAR zones, if not the robot would continoe moving forward.
If an obstacle was detected, it would be evaluated in which cone, and thereafter at what distance.
If the obstacle was in either of the center cones the robot, it would mean the obstacle is diretcly in the robots path meaning the robot will have to turn a lot.
The robot will have to turn less if an obstacle was detected in the middle cones, and even less if in the outer cones.
For the distance factor, an obstacle within the medium distance would result in maximum linear speed, but a close obstacle would mean a very significantly reduced linear speed.
To simplify the angle of the obstacle relaive to the robot would be the primary factor in determining the angular speed (i.e how much the robot turns), so the closer an obstacle is to the current path of the robot, the more it will turn.
In addition to that, the distance to said obstacle would be the primary factor in determining the linear speed (i.e how fast the robot moves forward), so the closer an obstacle is to the robot, the slower it will move to avoid collisions.
\begin{figure}[htbp]
    \centerline{\includegraphics[width=1.0\columnwidth]{Pictures/Flowchart.png}}
    \caption{Flowchart describing how the robot will act, depending on relative position of obstacle.}
    \label{sec:flowchart}
    \end{figure}

For the conrete implmentation we would use a bunch of if- and else statements, ressembeling the pseudo code in \href{sec:distances}{Fig. 8}.

\begin{figure}[htbp]
    \centerline{\includegraphics[width=0.6\columnwidth\hspace{-1.3cm}]{Pictures/Pseudo.png}}
    \caption{Pseudo navigation.}
    \label{sec:pseudo}
    \end{figure}

\subsection{RGB}
As the point of the robot was a Search and Rescue, we needed a way to detect when a victim was found, which was done by the aforementioned RGB sensor, which in this case was represented by red spots on the floor.
With this in mind, we placed the RGB sensor as close to the ground as possible in order to avoid as much light disturbance as possible.
Additionally, we also connected a white LED and placed it directly next to the RGB sensor as we experienced that it had some trouble distinguishing between colors if not properly lit.
The RGB was connected to the Raspberry Pi using four pins, two of these being the SDA (Serial Data Line) and SCL (Serial Clock Line)\cite{b4}. The SDA is the pin resposible for transfering raw data from the sensor to the Pi, where it is interpreted processed in order to extract the red, green, and blue values.
The SCL is simply a clock, which synchronizes data transfers over the I2C bus.

Furthermore, the sensor was connected to a GPIO pin, which would provide power to the RGB when the code was executed, and the fourth pin simply ground.
After connecting the RGB properly, we were able to read the different values at the clock rate provided by the SCL.
Since the victims were represented by red dots, we wrote our code in such a way that it would find a victim each time the red readings were above a certain threshold.

However, due to unavoidable uncertainties in measurement, we would have to manually descale the blue readings by a certain factor as these seemed to consistently be a bit higher than expected.
This was done by simply isolating the blue data and multiplying it by a factor we found fitting.
Eventually, the condition for finding a victim was as shown in Fig. 9.
If all these factors were present, an LED would light up and our victim counter would be incremented by 1.
We did end up adding a short delay to the detection as the sensor would otherwise be able to detect multiple victims from a single red spot, courtesy of the high clock speed.

\begin{figure}[htbp]
    \centerline{\includegraphics[width=0.9\columnwidth]{Pictures/rgbpseudo.png}}
    \caption{pseudo code showcasing the RGB logic}
    \label{sec:rgb}
    \end{figure}

\subsection{LED}
As mentioned in the RGB section, we had an LED light up each time a victim was found. This was done by simlpy connecting it to a different GPIO than the RGB and then having it light up whenever the victim conditions were met.
It would light up for a set amount of time and then turn off again afterwards.

The same thing was done for our collision counter, as this was also signified by an LED, only in a different color.
This would light up whenever the robot received a reading from the LiDAR below a certain threshold, corresponding to the length of the robot.
Again, the LED would light up for a short amount of time while also incrementing the variable responsible for counting the total amount of collisions during a run.
This detection was implemented completely independent from the other navigation code, resulting in us being able to detect a collision regardless of what stage of navigation the robot was currently in.

\section{Experimentation and Testing}
simplicity...
\section{Conclusion}

\section{Discussion}

\section{Personal Contributions}
Throughout the course, we generally worked in tandem in regards to the development and implementation of the robot.
Whenever a new step had to be taken, we discussed it between the two of us and came up with a rough idea of how to approach the problem and afterwards would work together on the solution, whether it was our navigation software or the integration of a new sensor or other external part.
When writing the report, the workload was divided in the follwing way:

Jakob wrote and was responsible for the following sections:
\begin{itemize}
    \item Section II - Specifications
    \item Subsection II.A - Software Setup
    \item Subsection III.A - LiDAR
    \item Subsection III.C - RGB
    \item Subsection III.D - LED
\end{itemize}

Simon wrote and was responsible for the following sections:
\begin{itemize}
    \item Section I - Introduction
    \item Section III - Methodology and Design
    \item Section III.B - Navigation
    \item Section IV - Experimentation and Testing
\end{itemize}

Sections V and VI along with the Abstract were written in collaboration to achieve a coalescence of thoughts and conclusions.
The same is true for the design of the figures used, along with the gathering of information.
\begin{thebibliography}{00}
\bibitem{b1} 'Turtlebot3 features', accessed 15 May 2024, available at: https://emanual.robotis.com/docs/en/platform/turtlebot3/features/
\bibitem{b2} 'Renesas RGB-sensor ISL29125 datasheet', accessed 15 May 2024, available at: https://www.alldatasheet.com/datasheet-pdf/pdf/1045936/RENESAS/ISL29125.html
\bibitem{b3} 'LDS-01 overview', accessed 15 May 2024, available at: https://www.robot-advance.com/EN/art-360-laser-distance-sensor-lds-01-2352.htm
\bibitem{b4} 'Using the I2C Bus', accessed 17 May 2024, available at: https://www.robot-electronics.co.uk/i2c-tutorial
\bibitem{b5} 'GitHub repository', accessed ... May 2024, available at: ...
\end{thebibliography}

\end{document}
